ARG UBUNTU_VERSION=24.04

# This needs to generally match the container host's environment.
ARG ROCM_VERSION=6.2
# Target the CUDA build image
ARG BASE_ROCM_DEV_CONTAINER=rocm/dev-ubuntu-${UBUNTU_VERSION}:${ROCM_VERSION}

ARG ROCM_DOCKER_ARCH="\
    gfx803 \
    gfx900 \
    gfx906 \
    gfx908 \
    gfx90a \
    gfx1010 \
    gfx1030 \
    gfx1100 \
    gfx1101 \
    gfx1102"

FROM ${BASE_ROCM_DEV_CONTAINER} AS build

WORKDIR /app

COPY . .

ENV LLAMA_CURL=1

RUN apt-get update && apt-get install -y libcurl4-openssl-dev cmake build-essential curl


RUN HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=${ROCM_DOCKER_ARCH} -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=ON 
RUN cmake --build build --config Release -- -j $(nproc)

    # locale settings for C
ENV LC_ALL=C.utf8
# Must be set to 0.0.0.0 so it can listen to requests from host machine, --host
ENV LLAMA_ARG_HOST=0.0.0.0
# --port
ENV LLAMA_ARG_PORT=8080
# --model-url
ENV LLAMA_ARG_MODEL_URL="https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
# -ngl, GPU layers
ENV LLAMA_ARG_N_GPU_LAYERS=99
ENV LLAMA_ARG_CTX_SIZE=0

HEALTHCHECK CMD [ "curl", "-f", "http://localhost:8080/health" ]
EXPOSE 8080

CMD ["/app/build/bin/llama-server"]