
services:
  llama:
    image: ghcr.io/gjuoun/llama.cpp-hosted/vulkan:latest
    ports:
      - "8080:8080"
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    volumes:
      - $HOME/.cache/llama.cpp:/root/.cache/llama.cpp
    environment:
      - LLAMA_ARG_FLASH_ATTN=true
      - LLAMA_ARG_ALIAS=qwen2.5-3b
      - LLAMA_ARG_N_GPU_LAYERS=37
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8080
      - LLAMA_ARG_CTX_SIZE=0
      - LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf

networks:
  default:
    name: llama-network